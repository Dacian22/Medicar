from typing import Any, Dict, List, Optional
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langserve import RemoteRunnable

class LLama(LLM):
    model: RemoteRunnable

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        
        """
        Invokes the underlying model with the provided prompt and returns the response.

        Args:
            prompt (str): The input text prompt to be passed to the model for generating a response.
            stop (Optional[List[str]]): An optional list of stop tokens to indicate where the generation should stop.
            run_manager (Optional[CallbackManagerForLLMRun]): Optional manager for handling callbacks during the model run.
            **kwargs (Any): Additional keyword arguments that may be passed to the model.

        Returns:
            str: The content of the response generated by the model based on the input prompt.
        """        
        
        answer=self.model.invoke(prompt)

        return answer.content

    

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """
        Property that returns a dictionary of identifying parameters for the model.

        Returns:
            Dict[str, Any]: A dictionary containing metadata that identifies the model being used.
            In this case, it includes the model name.
        """

        return {
            "model_name": "LLama Uni Server",
        }


    @property
    def _llm_type(self) -> str:
        """
        Property that returns the type of the language model (LLM).

        Returns:
            str: A string representing the type of the LLM. In this case, it returns "custom" 
            to indicate that this is a custom implementation of a language model.
        """
         
        return "custom"