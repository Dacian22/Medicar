{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "sys.path.append('./_01_Simulation/')\n",
    "\n",
    "import _01_Simulation.LLM_Edge_Usability\n",
    "import _01_Simulation.LLM_Dynamic_Weights\n",
    "import _01_Simulation.LLM_MetaModel\n",
    "import _01_Simulation.LLM_Function_Calling"
   ],
   "id": "3d419cfad8ae5e35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load .env file\n",
    "dotenv.load_dotenv()"
   ],
   "id": "1f561e7e91c8d35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Deactivate deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ],
   "id": "fdc8719023c3029e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Edge usability",
   "id": "ad46b5e6f445a87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Params (currently only considers openai models\n",
    "approaches = [\"zeroshot\", \"fewshot\"]"
   ],
   "id": "df76334181794a14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load evaluation csv\n",
    "df_evaluation = pd.read_csv(os.path.join(os.getenv('RESOURCES'), 'EvaluationDataset.csv'), sep=\";\")"
   ],
   "id": "42a0d76d145bb7da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_evaluation.head()",
   "id": "ae7913f8d99a6a29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO Remove this statement to evaluate the whole dataset. This will induce extensive costs for API usage.\n",
    "df_evaluation = df_evaluation.iloc[:2]"
   ],
   "id": "ef7371ab95fab406",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Invoke GPT-3.5 to obtain the predictions for edge-usability\n",
    "\n",
    "for approach in approaches:\n",
    "    print(f\"Approach: {approach}\")\n",
    "    predictions = []\n",
    "    for index, row in df_evaluation.iterrows():\n",
    "        model_output = _01_Simulation.LLM_Edge_Usability.invoke_llm(row[\"action\"], model_type=\"openai\", approach=approach)\n",
    "        prediction = _01_Simulation.LLM_Edge_Usability.parse_response(model_output)\n",
    "        predictions.append(prediction)\n",
    "        print(f\"Finished {index+1}/{len(df_evaluation)}\")\n",
    "    \n",
    "    df_evaluation[\"prediction\"] = predictions\n",
    "    df_evaluation.to_csv(os.path.join(os.getenv('RESULTS'), f'eval-res-edge-usability-openai-{approach}-{datetime.datetime.now().isoformat()}.csv'), sep=\";\", index=False)"
   ],
   "id": "cbfa9e52b0bfa3c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Dynamic Edge Weights",
   "id": "d594d55fc6a6d1bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Params (currently only considers openai models\n",
    "approaches = [\"zeroshot\", \"fewshot\"]"
   ],
   "id": "908861a429177869",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load evaluation csv\n",
    "df_evaluation = pd.read_csv(os.path.join(os.getenv('RESOURCES'), 'EvaluationDataset-dynamic-edge-weight.csv'), sep=\";\")"
   ],
   "id": "a69b7fc418cee727",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_evaluation.head()",
   "id": "e3094764c2e2a155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO Remove this statement to evaluate the whole dataset. This will induce extensive costs for API usage.\n",
    "df_evaluation = df_evaluation.iloc[:2]"
   ],
   "id": "ad5087da3ed3776",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_evaluation",
   "id": "53d5beb969070780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Invoke GPT-3.5 to obtain the predictions for dynamic-edge-weights\n",
    "\n",
    "for approach in approaches:\n",
    "    print(f\"Approach: {approach}\")\n",
    "    predictions = []\n",
    "    result_types = []\n",
    "    for index, row in df_evaluation.iterrows():\n",
    "        model_output, _, result_type = _01_Simulation.LLM_Dynamic_Weights.invoke_llm_chain(row[\"action\"], model_type=\"openai\", approach=approach)\n",
    "        prediction = _01_Simulation.LLM_Dynamic_Weights.parse_output_weights(model_output)\n",
    "        predictions.append(prediction)\n",
    "        result_types.append(result_type)\n",
    "        print(f\"Finished {index+1}/{len(df_evaluation)}\")\n",
    "    \n",
    "    df_evaluation[\"prediction\"] = predictions\n",
    "    df_evaluation[\"result_type\"] = result_types\n",
    "    df_evaluation.to_csv(os.path.join(os.getenv('RESULTS'), f'eval-res-dynamic-openai-{approach}-{datetime.datetime.now().isoformat()}.csv'), sep=\";\", index=False)"
   ],
   "id": "e47ea24722a02d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Functioncalling",
   "id": "2379f604a55a73e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load evaluation csv\n",
    "df_evaluation = pd.read_csv(os.path.join(os.getenv('RESOURCES'), 'EvaluationDataset-node-incidents.csv'), sep=\";\")"
   ],
   "id": "1d8c1c7fae1a0785",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_evaluation.head()",
   "id": "8aaefd92e6a84ddc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO Remove this statement to evaluate the whole dataset. This will induce extensive costs for API usage.\n",
    "df_evaluation = df_evaluation.iloc[:2]"
   ],
   "id": "d065e6bc17115860",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "node_incidents = []\n",
    "for index, row in df_evaluation.iterrows():\n",
    "    node_incident = _01_Simulation.LLM_Function_Calling.invoke_llm(row[\"examples\"])\n",
    "    node_incidents.append(', '.join(node_incident))\n",
    "    print(f\"Finished {index+1}/{len(df_evaluation)}\")\n",
    "\n",
    "df_evaluation[\"node_incidents\"] = node_incidents\n",
    "\n",
    "df_evaluation.to_csv(os.path.join(os.getenv('RESULTS'), f'eval-res-function-calling-{datetime.datetime.now().isoformat()}.csv'), sep=\";\", index=False)"
   ],
   "id": "1da880e8bac44743",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Meta-Model",
   "id": "929f4322f06811b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load evaluation csv\n",
    "df_evaluation = pd.read_csv(os.path.join(os.getenv('RESOURCES'), 'EvaluationDataset-metamodel.csv'), sep=\";\")"
   ],
   "id": "6cd4000c9ba96202",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_evaluation.head()",
   "id": "e13a07fae22e00e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO Remove this statement to evaluate the whole dataset. This will induce extensive costs for API usage.\n",
    "df_evaluation = df_evaluation.iloc[:2]"
   ],
   "id": "926879033cdbe6f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_usabilities = []\n",
    "output_dynamics = []\n",
    "output_lengths = []\n",
    "output_times = []\n",
    "output_nodes = []\n",
    "output_nodes_times = []\n",
    "for index, row in df_evaluation.iterrows():\n",
    "    output_usability, output_dynamic, output_length, output_time, output_node, output_nodes_time, _ = _01_Simulation.LLM_MetaModel.invoke_llm(row[\"examples\"])\n",
    "    output_usabilities.append(output_usability)\n",
    "    output_dynamics.append(output_dynamic)\n",
    "    output_lengths.append(output_length)\n",
    "    output_times.append(output_time)\n",
    "    output_nodes.append(output_node)\n",
    "    output_nodes_times.append(output_nodes_time)\n",
    "    print(f\"Finished {index+1}/{len(df_evaluation)}\")\n",
    "\n",
    "df_evaluation[\"output_usability\"] = output_usabilities\n",
    "df_evaluation[\"output_dynamic\"] = output_dynamics\n",
    "df_evaluation[\"output_length\"] = output_lengths\n",
    "df_evaluation[\"output_time\"] = output_times\n",
    "df_evaluation[\"output_node\"] = output_nodes\n",
    "\n",
    "df_evaluation.to_csv(os.path.join(os.getenv('RESULTS'), f'eval-res-metamodel-{datetime.datetime.now().isoformat()}.csv'), sep=\";\", index=False)"
   ],
   "id": "4a85f6f4eb5c8a88",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
